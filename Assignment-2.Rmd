---
title: "Assignment 2"
subtitle: "EC 607: Health and Crime"
author: "Brock Wilson, Emmett Saulnier, and Tanner Bivins"
date: "5/31/2021"
output: html_document
---

```{R}
library(pacman)

p_load(data.table, dtplyr, dplyr, ggplot2, 
       knitr, kableExtra, fixest, modelsummary, 
       broom, purrr, Synth, qte)
```

## 1. How does recreational marijuana legalization affect crime.

Several states have legalized marijuana for recreational use. The goal in this study, is to estimate whether recreational marijuana legalization has increased violent crime, based on data from the supplemental homicides reports (SHR) (shr.csv file).

Start with micro level data from SHR. In the dataset, I would you like to keep only murders involving 3 or fewer victims to avoid mass shootings or other crimes (terrorism events like 9/11). So keep only the murders with additional_victim_count of 3 or fewer. Aggregate up to the state by year level. Then merge with population data from the state_pop.csv file. Create a variable murder_pop which is murders/(pop/100000). The average murder rate in the US is around 5 recently. Inspect the time series for each state. You’ll find you should drop 3 state_fips regions due to under-reporting/missing data (you find which 3).

The following states legalized recreational marijuana recently.

Colorado – 2014 

Washington -2014 

Oregon – 2015 

Alaska – 2015 

Nevada - 2017

```{R clean}

# Reading data
shr = fread("data/shr.csv")
state_pop = fread("data/statepop.csv")

# Removing murders with more than 3 victims and those without FIPS codes
shr_final = shr[shr$additional_victim_count <= 2 & !is.na(fips_state_code),]

# Aggregating murders to state, year level
murder_rates = 
  shr_final[,
      .(victim_count = .N), 
      by = .(fips_state_code,year)
    ]
setnames(murder_rates, old = "fips_state_code", new = "state_fips")

# Merging with state population data 
state_pop_murder = 
  merge(
    state_pop[year < 2018],
    murder_rates, 
    by = c("state_fips","year"), 
    all.x = TRUE
  )

# Calculating murder_pop
state_pop_murder[, murder_pop := victim_count/(pop/100000)]

# Checking to see if murder_pop is ~5 on avg "recently"
ggplot(
  data = state_pop_murder[,
            .(avg_murder_rate = mean(murder_pop, na.rm = TRUE)), 
            by = "year"
          ],
  aes(x= year, y = avg_murder_rate)) +
  geom_line(color = "blue") +
 # geom_hline(yintercept = 5, linetype = "dashed")+
  theme_classic() +
  labs(title = "Average Murder Rate in the US",
       x= "Year",y="Avg Murder Rate (Victims/(Pop/100K))")
  

# Looking for missing data
state_pop_murder[is.na(murder_pop), .(num_na = .N), by = state] %>%
  .[num_na > 0] %>% 
  kable(col.names = c("State","Years of Missing Data")) %>%
  kable_styling(full_width = FALSE)

# Checking Alabama
ggplot(data = state_pop_murder[state_fips == 1], aes(x= year, y = murder_pop)) +
  geom_line(color = "blue") +
  theme_classic() +
  labs(title = "Murder Rate in Alabama",
       x= "Year",y="Murder Rate (Victims/(Pop/100K))")

# DC and Florida missing lots of data, Alabama reporting almost nothing after 2010
state_pop_murder_final = state_pop_murder[!(state_fips %in% c(1, 11, 12)), ]

```


```{r, eval = FALSE, echo = FALSE}
# DPLYR Version

shr_final = tibble(shr_final)

murder_rates = shr_final %>%
  group_by(fips_state_code, year) %>%
  summarise(victim_count = sum(additional_victim_count) + n())

colnames(murder_rates)[1] = "state_fips"

# Loading State Pop


colnames(state_pop)[6] = "state_fips"

# Merge data

state_pop_murder = state_pop %>%
  left_join(murder_rates, by = c("state_fips", "year"))

state_pop_murder$murder_pop = state_pop_murder$victim_count/(state_pop_murder$pop/100000)

# Inspecting for NAs/Underreporting

state_pop_murder %>%
  group_by(state_fips) %>%
  summarise(sumNA = sum(is.na(murder_pop))) %>%
  arrange(-sumNA)

# We have 2000-2020 data; this suggests that State 12 is missing all of its data (Florida)

# State 11 (D.C.) is missing a significant amount of data

state_pop_murder %>%
  group_by(state_fips) %>%
  summarise(meanMurder = mean(murder_pop, na.rm = TRUE)) %>%
  arrange(-meanMurder)

# Need to find one more state

ggplot(data = state_pop_murder, aes(x = year, y = murder_pop, color = state)) +
  geom_line()

# State 1 (Alabama) is most likely misreporting its murder rate: there is a huge discontinuous drop in the rate

state_pop_murder = data.table(state_pop_murder)

state_pop_murder_final = state_pop_murder[!(state_fips %in% c(1, 11, 12)), ]
```

### A. 

Create a dummy variable that indicates whether recreational marijuana is available within a state. Now run a regression with log(murder_pop) on the left. On the right hand side, control for state fixed effects, year fixed effects, and the recreational marijuana dummy. What is the coefficient, and upper and lower bounds. Does marijuana contribute to reefer madness as suggested by Alex Berenson in his recent book?

#### SOLUTION


```{R}

# Creating dummy variable for recreational marijuana
state_pop_murder_final[,
   mj_legal := ifelse(
                   (year>=2015 & state == "OREGON") |
                   (year>=2014 & state == "WASHINGTON")|
                   (year>=2014 & state == "COLORADO")|
                   (year>=2015 & state == "ALASKA") |
                   (year>=2017 & state == "NEVADA"),
                  1, 0)
]

# Regressions 
reg1 = feols(
  fml = log(murder_pop) ~ mj_legal | state_fips + year,
  data = state_pop_murder_final
)

modelsummary(reg1)

# 95% confidence interval on coefficient 
mj_coef_ub = round(reg1$coefficients[1] + 1.96*reg1$se[1], digits = 3)
mj_coef_lb = round(reg1$coefficients[1] - 1.96*reg1$se[1], digits = 3)

```


The 95% confidence interval for the effect of recreational marijuana legalization on murder rate is (`r mj_coef_lb`,`r mj_coef_ub`). This indicates that legalization does not contribute to "reefer madness". The point estimate on the coefficient is negative suggesting that legalization of marijuana decreases violent crime.


### B. 

Now I want you to collapse your data to 3 regions. Colorado, Washington, and the rest of the country. Create a connected line plot with murder on the y-axis and year on the x- axis. Create a vertical line marking 2014. What broadly happens to average murder rates in Colorado and Washington, the first legalizers? What happens in the rest of the country? Do the trends in the Colorado and Washington match those in the rest of the country?

#### SOLUTION

The average murder rate in Colorado and Washington increases significantly in 2014 (Washington less so than Colorado). The rest of the country also has a large increase in the average murder rate in 2014. The trends in Colorado and Washington seem to follow the average murder rate in the rest of the US closely, the only exception is that Colorado had a large spike in murders in 2004. This is reassuring because using the rest of the US for a synthetic control seems acceptable.

```{R}

# Creating 3 regions 
state_pop_murder_final[,
  region := ifelse(!(state %in% c("COLORADO","WASHINGTON")), "Rest of Country", state)
]

# Aggregating data by region 
region_pop_murder = 
  state_pop_murder_final[,
    .(tot_victim_count = sum(victim_count),
      tot_pop = sum(pop)),
    by = .(region,year)
  ] %>%
  .[,
    .(region, year, avg_murder_pop = tot_victim_count/(tot_pop/100000))
  ]

ggplot(data = region_pop_murder, aes(x = year, y = avg_murder_pop)) +
  geom_line(aes(color = region)) + 
  geom_vline(xintercept = 2014, linetype = "dashed")+
  theme_classic() + 
  labs(title = "Annual Murder Rate per 100K People by Region",
       x = "Year",
       y = "Murders/(Population/100K))",
       color = "Region")


```

### C. 

Now use the state by year data again. Drop California, Oregon, Nevada, and Alaska (they legalized early, but not enough for post treatment data). Use a synthetic control approach to estimate the effect of marijuana legalization for both Colorado, and Washington. Match on the lagged murder rate per 100,000 for each year 2000, 2001, 2002, etc. through 2013. How does the murder rate in Colorado and Washington compare to its synthetic control average for 2014-2017?

#### SOLUTION

For Colorado, the synthetic version had approximately the same murder rate as Colorado. This suggests legalization did little to the murder rate in Colorado. For Washington, the synthetic control suggests there was a spike in murders that Washington avoided. This suggests that Washington may have had a lower murder rate due to legalization.

```{R, echo = FALSE}
p_load(optimx, kernlab)

quietsynth = function (data.prep.obj = NULL, X1 = NULL, X0 = NULL, Z0 = NULL, 
    Z1 = NULL, custom.v = NULL, optimxmethod = c("Nelder-Mead", 
        "BFGS"), genoud = FALSE, quadopt = "ipop", Margin.ipop = 5e-04, 
    Sigf.ipop = 5, Bound.ipop = 10, verbose = FALSE, ...) 
{
    if (is.null(data.prep.obj) == FALSE) {
        X1 <- data.prep.obj$X1
        Z1 <- data.prep.obj$Z1
        X0 <- data.prep.obj$X0
        Z0 <- data.prep.obj$Z0
    }
    else {
        cat("X1,X0,Z1,Z0 were individually input (not dataprep object.)\n\n")
    }
    store <- list(X1 = X1, X0 = X0, Z1 = Z1, Z0 = Z0)
    for (i in 1:4) {
        if (is.null(store[[i]])) {
            stop(paste("\n", names(store)[i], "is missing \n"))
        }
        if (sum(is.na(store[[i]])) > 0) {
            stop(paste("\n NAs in", names(store)[i], "\n"))
        }
        if (is.matrix(store[[i]]) == FALSE) {
            stop(paste("\n", names(store)[i], "is not a matrix object\n"))
        }
    }
    if (ncol(X1) != 1) {
        stop("\n Please specify only one treated unit: X1 has to have ncol= 1")
    }
    if (ncol(Z1) != 1) {
        stop("\n Please specify only one treated unit: Z1 has to have ncol= 1")
    }
    if (ncol(X0) < 2) {
        stop("\n Please specify at least two control units: X0 has to have ncol >= 2 ")
    }
    if (ncol(Z0) < 2) {
        stop("\n Please specify only one treated unit: Z0 has to have ncol >= 2")
    }
    if (nrow(Z0) != nrow(Z1)) {
        stop("\n Different number of periods for treated and controls: nrow(Z0) unequal nrow(Z1)")
    }
    if (nrow(X0) != nrow(X1)) {
        stop("\n Different number of predictors for treated and controls: nrow(X0) unequal nrow(X1)")
    }
    if (nrow(X0) == 0) {
        stop("No predictors specified. Please specify at least on predictor")
    }
    if (nrow(Z0) == 0) {
        stop("No periods specified for Z1 and Z0. Please specify at least on period")
    }
    if (0 %in% apply(X0, 1, sd)) {
        stop("\n At least one predictor in X0 has no variation across control units. Please remove this predictor.")
    }
    nvarsV <- dim(X0)[1]
    big.dataframe <- cbind(X0, X1)
    divisor <- sqrt(apply(big.dataframe, 1, var))
    scaled.matrix <- t(t(big.dataframe) %*% (1/(divisor) * diag(rep(dim(big.dataframe)[1], 
        1))))
    X0.scaled <- scaled.matrix[, c(1:(dim(X0)[2]))]
    if (is.vector(X0.scaled) == TRUE) {
        X0.scaled <- t(as.matrix(X0.scaled))
    }
    X1.scaled <- scaled.matrix[, dim(scaled.matrix)[2]]
    if (is.null(custom.v) & nrow(X0) != 1) {
        cat("\n****************", "\n searching for synthetic control unit  \n", 
            "\n")
        if (genoud == TRUE) {
            require(rgenoud)
            cat("\n****************", "\n genoud() requested for optimization\n", 
                "\n")
            rgV.genoud <- genoud(fn.V, nvarsV, X0.scaled = X0.scaled, 
                X1.scaled = X1.scaled, Z0 = Z0, Z1 = Z1, quadopt = quadopt, 
                margin.ipop = Margin.ipop, sigf.ipop = Sigf.ipop, 
                bound.ipop = Bound.ipop)
            SV1 <- rgV.genoud$par
            cat("\n****************", "\n genoud() finished, now running local optimization using optim()\n", 
                "\n")
        }
        else {
            SV1 <- rep(1/nvarsV, nvarsV)
        }
        all.methods <- FALSE
        if (sum(optimxmethod %in% c("All")) == 1) {
            all.methods <- TRUE
        }
        rgV.optim.1 <- optimx(par = SV1, fn = fn.V, gr = NULL, 
            hess = NULL, method = optimxmethod, itnmax = NULL, 
            hessian = FALSE, control = list(kkt = FALSE, starttests = FALSE, 
                dowarn = FALSE, all.methods = all.methods), X0.scaled = X0.scaled, 
            X1.scaled = X1.scaled, Z0 = Z0, Z1 = Z1, quadopt = quadopt, 
            margin.ipop = Margin.ipop, sigf.ipop = Sigf.ipop, 
            bound.ipop = Bound.ipop)
        if (verbose == TRUE) {
            print(rgV.optim.1)
        }
        rgV.optim.1 <- collect.optimx(rgV.optim.1, "min")
        Xall <- cbind(X1.scaled, X0.scaled)
        Xall <- cbind(rep(1, ncol(Xall)), t(Xall))
        Zall <- cbind(Z1, Z0)
        Beta <- try(solve(t(Xall) %*% Xall) %*% t(Xall) %*% t(Zall), 
            silent = TRUE)
        if (inherits(Beta, "try-error")) {
            rgV.optim <- rgV.optim.1
        }
        else {
            Beta <- Beta[-1, ]
            V <- Beta %*% t(Beta)
            SV2 <- diag(V)
            SV2 <- SV2/sum(SV2)
            rgV.optim.2 <- optimx(par = SV2, fn = fn.V, gr = NULL, 
                hess = NULL, method = optimxmethod, itnmax = NULL, 
                hessian = FALSE, control = list(kkt = FALSE, 
                  starttests = FALSE, dowarn = FALSE, all.methods = all.methods), 
                X0.scaled = X0.scaled, X1.scaled = X1.scaled, 
                Z0 = Z0, Z1 = Z1, quadopt = quadopt, margin.ipop = Margin.ipop, 
                sigf.ipop = Sigf.ipop, bound.ipop = Bound.ipop)
            if (verbose == TRUE) {
                print(rgV.optim.2)
            }
            rgV.optim.2 <- collect.optimx(rgV.optim.2, "min")
            if (verbose == TRUE) {
                cat("\n Equal weight loss is:", rgV.optim.1$value, 
                  "\n")
                cat("\n Regression Loss is:", rgV.optim.2$value, 
                  "\n")
            }
            if (rgV.optim.1$value < rgV.optim.2$value) {
                rgV.optim <- rgV.optim.1
            }
            else {
                rgV.optim <- rgV.optim.2
            }
        }
        solution.v <- abs(rgV.optim$par)/sum(abs(rgV.optim$par))
    }
    else {
      
        if (nrow(X0) == 1) {
            custom.v <- 1
        }
        else {
            cat("\n****************", "\n v weights supplied manually: computing synthtic control unit \n", 
                "\n\n")
            if (length(custom.v) != nvarsV) {
                stop("custom.V misspecified: length(custom.V) != nrow(X1)")
            }
            if (mode(custom.v) != "numeric") {
                stop("custom.V must be numeric")
            }
        }
        rgV.optim <- NULL
        solution.v <- abs(custom.v)/sum(custom.v)
    }
    V <- diag(x = as.numeric(solution.v), nrow = nvarsV, ncol = nvarsV)
    H <- t(X0.scaled) %*% V %*% (X0.scaled)
    a <- X1.scaled
    c <- -1 * c(t(a) %*% V %*% (X0.scaled))
    A <- t(rep(1, length(c)))
    b <- 1
    l <- rep(0, length(c))
    u <- rep(1, length(c))
    r <- 0
    if (quadopt == "ipop") {
        res <- ipop(c = c, H = H, A = A, b = b, l = l, u = u, 
            r = r, margin = Margin.ipop, maxiter = 1000, sigf = Sigf.ipop, 
            bound = Bound.ipop)
        solution.w <- as.matrix(primal(res))
    }
    else {
        if (quadopt == "LowRankQP") {
            res <- LowRankQP(Vmat = H, dvec = c, Amat = A, bvec = 1, 
                uvec = rep(1, length(c)), method = "LU")
            solution.w <- as.matrix(res$alpha)
        }
    }
    rownames(solution.w) <- colnames(X0)
    colnames(solution.w) <- "w.weight"
    names(solution.v) <- rownames(X0)
    loss.w <- t(X1.scaled - X0.scaled %*% solution.w) %*% V %*% 
        (X1.scaled - X0.scaled %*% solution.w)
    loss.v <- t(Z1 - Z0 %*% as.matrix(solution.w)) %*% (Z1 - 
        Z0 %*% as.matrix(solution.w))
    loss.v <- loss.v/nrow(Z0)
    optimize.out <- list(solution.v = solution.v, solution.w = solution.w, 
        loss.v = loss.v, loss.w = loss.w, custom.v = custom.v, 
        rgV.optim = rgV.optim)
    return(invisible(optimize.out))
}
```


```{R}
# Great paper on syn control: https://economics.mit.edu/files/17847

# Dropping Cali, Oregon, Nevada, Alaska
state_pop_murder_synthetic = state_pop_murder_final[!(state_fips %in% c(6, 41, 32, 2))]

# Creating lagged murder_pop
lagged_pop_murder = 
  state_pop_murder_final[,
    .(state_fips, murder_pop, year = year + 1)
  ]

state_pop_murder_synthetic =
  state_pop_murder_synthetic %>%
  merge(
    setnames(lagged_pop_murder, old = "murder_pop", new = "lagged_murder_pop"),
    by = c("state_fips","year"),
    all.x = TRUE
  )


# Function to run synthetic control for a specified state_fips
synthcontrol = function(state_fips_treated, plot = FALSE, dataframe = TRUE){

  # Dataprep 
  dataprep = dataprep(
    foo = state_pop_murder_synthetic, #dataset
    predictors = c("lagged_murder_pop"), 
    predictors.op = "mean",
    time.predictors.prior = c(2001:2013), # pre-intervention window
    dependent = "murder_pop", # outcome variable
    unit.variable = "state_fips", # identify our id variable
    unit.names.variable = "state", # identify our name variable
    time.variable = "year", # identify our time period variable
    treatment.identifier = state_fips_treated, # id for treated unit
    controls.identifier =  unique(state_pop_murder_synthetic[!(state_fips %in% c(8,53,state_fips_treated)), state_fips]), # vector of id's for donor pool
    time.optimize.ssr = c(2001:2014), # pre-treatment period and the treatment year.
    time.plot = c(2001:2017) # periods to plot
  )
  
  # Now we have our data ready in the form of a list. We have all the matrices we need to run synth()
  # Our output from the synth() function will be a list that includes our optimal weight matrix W*
  
  synth_out_treated = (dataprep %>% quietsynth(verbose = FALSE))
  
  # From here, we can plot the treatment variable and the synthetic control using Synth's plot function.
  # The variable tr.intake is an optional variable if you want a dashed vertical line where the intervention takes place.
  
  if(plot){
    synth_out_treated %>% path.plot(dataprep.res = dataprep, tr.intake = 2014, Ylab = c("Average Murder Rate"), Main = as.character(state_pop_murder_synthetic[state_fips == state_fips_treated & year == 2002, state]))
  }
    path.plot
  # Finally, we can construct our synthetic control variable if we wanted to conduct difference-in-difference analysis on it to estimate the treatment effect.
  
  synth_control_treated = dataprep$Y0plot %*% synth_out_treated$solution.w
  
  # Pre Treatment Error
  preerror = mean(
    (as.vector(synth_control_treated[1:13]) -
       as.vector(state_pop_murder_synthetic[state_fips==state_fips_treated & 
                                              year > 2000 & 
                                              year < 2014, 
                                            murder_pop])
     )^2)
  
  # Post Treatment Error
  posterror = mean(
    (as.vector(synth_control_treated[14:17]) - 
      as.vector(state_pop_murder_synthetic[state_fips==state_fips_treated & 
                                             year > 2013 & 
                                             year < 2018, 
                                           murder_pop])
     )^2)
  
  if(dataframe){
    return(data.frame(state_fips_treated, preerror, posterror))
  }
}

# Running the synthetic control function
synthcontrol(8, TRUE, FALSE) # Colorado

synthcontrol(53, TRUE, FALSE) # Washington

```

### D. 

Use randomization inference to conduct hypothesis testing. To do so, reestimate the synthetic control approach for each state not treated excluding the two treatment states (and the other later adopting states). Estimate the mean squared error (MSE) for the placebo states, and for the treated states, both before and after recreational marijuana legalization. Now generate a ratio of postMSE/preMSE. How does CO and Washington compare to the distribution. This is your empirical p-value. Can you reject the $H_0$ that marijuana legalization did not increase crime? Why or why not?

#### SOLUTION

```{r partd, message =F}

# Running synthetic control for all states
output = map_dfr(unique(state_pop_murder_synthetic[, state_fips]), synthcontrol)

# Calculating ratio of post and pre errors
output$ratio = output$posterror/output$preerror

# Getting ranks for empirical p-values
output %<>%
  tibble() %>%
  arrange(-ratio) %>%
  mutate(rnk = row_number(),
         region = case_when(
            state_fips_treated == 8 ~ "Colorado",
            state_fips_treated == 53 ~ "Washington",
            TRUE ~ "Rest of Country"
          )
        )


output = data.table(output)

# Plotting histogram
ggplot(data = output, aes(x = ratio)) + 
  geom_histogram() + 
  geom_vline(xintercept = output[state_fips_treated == 8,ratio], color = "red")+ 
  geom_vline(xintercept = output[state_fips_treated == 53,ratio], color = "blue") + 
  theme_classic() +
  labs(title = "Distribution of the Ratio of Post MSE to Pre MSE",
       subtitle = "Colorado is the red line and Washington is the blue line",
        x = "Ratio of Post MSE to Pre MSE",
        y = "Number of States")

# Colorado Empirical P Value
co_pval = output[state_fips_treated == 8, rnk]/(nrow(output)-1)
# Washington Empirical P Value
wa_pval = output[state_fips_treated == 53, rnk]/(nrow(output)-1)
```

We fail to reject the null that marijuana did not increase crime with p-values of `r co_pval` for Colorado and `r wa_pval` for Washington. This is because the ratio of PostMSE/PreMSE is low and that implies that crime did not spike post treatment. Altogether this suggests that marijuana legalization does not increase murder rates.

## 2. Quantile Regressions

Reopen the dataset you already downloaded for replication 1, LATE_BETTER_THAN_NEVER.csv. Now make a histogram as you did before for the distribution of individual treatment effects. Make sure you put vertical lines at the 5th percentile of treatment effects, the median, and the 95 percentile of treatment effects.

Now I want you to estimate quantile treatment effects. You can either do this on your own, or use the QTE package. https://cran.r-project.org/web/packages/qte/vignettes/R- QTEsWrapper.pdf

Estimate QTE’s for every percentile from the 1 percentile to the 99th percentile. Do QTE’s allow you to recover the distribution of individual treatment effects?

#### SOLUTION

No, quantile regression does not allow you to recover the distribution of individual treatment effects; it returns the average treatment effect over the distribution of outcomes. The individual treatment effects are not recoverable since we must know both Y1 and Y0, where we only ever observe one or the other. The histogram below shows each individual's treatment effect, with the dashed red lines representing the 1st and 99th percentile of effects. These individual treatment effects do not map to the quantile treatment effects shown in the second plot.

```{R}

# Reading data
bltn = fread("data/Better_Late_Than_Never.csv")

# Plotting histogram
ggplot(data = bltn, aes(x = T)) +
  geom_histogram(binwidth = 0.1) +
  geom_vline(xintercept = quantile(bltn$T, c(0.05)), color = "red", linetype = "dashed")  + 
  geom_vline(xintercept = quantile(bltn$T, c(0.95)), color = "red", linetype = "dashed") +
  geom_vline(xintercept = quantile(bltn$T, c(0.5)), color = "red") + 
  labs(title = "Distribution of Treatment Effects",
       subtitle = "5th, 50th, and 95th percentiles designated by red vertical lines",
       x = "Treatment Effect",
       y = "Count") + 
  theme_classic()
 
# Running quantile regression using qte package
ite = ci.qtet(formla = y ~ D, data = bltn, probs = seq(0.01,0.99,0.01))

# Plotting results
ggqte(ite)
```

